# Implementation Plan: RAG Website Ingestion Pipeline

**Branch**: `001-rag-ingestion-pipeline` | **Date**: 2025-01-03 | **Spec**: [link](./spec.md)
**Input**: Feature specification from `/specs/001-rag-ingestion-pipeline/spec.md`

**Note**: This template is filled in by the `/sp.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Implementation of a RAG ingestion pipeline that crawls Docusaurus-based technical book websites, extracts clean text content, generates embeddings using Cohere models, and stores vectors with metadata in Qdrant Cloud. The pipeline will be idempotent and re-runnable without duplication, with deterministic chunking and stable chunk IDs.

## Technical Context

**Language/Version**: Python 3.11
**Primary Dependencies**: requests, beautifulsoup4, cohere, qdrant-client, python-dotenv, uv (for project management)
**Storage**: Qdrant Cloud (vector storage), local file system for configuration
**Testing**: pytest for unit and integration tests
**Target Platform**: Linux/Mac/Windows server environment
**Project Type**: Single backend project with CLI interface
**Performance Goals**: Process typical technical book (100-500 pages) within 2 hours, achieve 99% URL crawl success rate
**Constraints**: <200MB memory usage during processing, handle rate limiting from APIs, offline-capable configuration
**Scale/Scope**: Support books up to 1000 pages, handle 10,000+ text chunks per book

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

The implementation aligns with the constitution requirements:
- Uses specified technology stack (Python, Qdrant Cloud for vector storage)
- Follows specification-first development approach
- Maintains production-grade standards with proper error handling
- Implementation will use Python with appropriate libraries for web crawling, text processing, and vector storage
- Will include proper testing and documentation

## Project Structure

### Documentation (this feature)

```text
specs/001-rag-ingestion-pipeline/
├── plan.md              # This file (/sp.plan command output)
├── research.md          # Phase 0 output (/sp.plan command)
├── data-model.md        # Phase 1 output (/sp.plan command)
├── quickstart.md        # Phase 1 output (/sp.plan command)
├── contracts/           # Phase 1 output (/sp.plan command)
└── tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)

```text
backend/
├── pyproject.toml          # Project configuration with dependencies
├── main.py                 # Main entry point orchestrating the full pipeline
├── .env                    # Environment variables (gitignored)
├── .env.example            # Example environment variables
├── requirements.txt        # Dependencies (generated by uv)
├── src/
│   ├── crawler/            # URL discovery and web crawling functionality
│   │   ├── __init__.py
│   │   ├── docusaurus_crawler.py    # Docusaurus-specific crawling logic
│   │   └── url_discovery.py         # URL discovery and validation
│   ├── text_processor/     # Text extraction and processing
│   │   ├── __init__.py
│   │   ├── content_extractor.py     # Clean text extraction from HTML
│   │   ├── chunker.py               # Deterministic text chunking
│   │   └── hierarchy_preserver.py   # Document hierarchy preservation
│   ├── embeddings/         # Embedding generation functionality
│   │   ├── __init__.py
│   │   ├── cohere_client.py         # Cohere API integration
│   │   └── embedding_generator.py   # Embedding generation and management
│   ├── storage/            # Vector storage functionality
│   │   ├── __init__.py
│   │   ├── qdrant_client.py         # Qdrant Cloud integration
│   │   └── vector_storage.py        # Vector storage and retrieval
│   └── utils/              # Utility functions
│       ├── __init__.py
│       ├── config_loader.py         # Configuration loading
│       ├── id_generator.py          # Stable chunk ID generation
│       └── retry_handler.py         # Error handling and retry logic
└── tests/
    ├── unit/
    │   ├── test_crawler/
    │   ├── test_text_processor/
    │   ├── test_embeddings/
    │   └── test_storage/
    ├── integration/
    │   └── test_pipeline.py         # End-to-end pipeline tests
    └── fixtures/                    # Test fixtures and sample data
```

**Structure Decision**: Backend project structure selected with clear separation of concerns. The main entry point (main.py) orchestrates the full pipeline: URL discovery → text extraction → deterministic chunking → embedding generation → vector storage. The project will be initialized with uv for modern Python package management.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| Multiple modules instead of single file | Maintainability and testability | Single file would be difficult to maintain and test as complexity grows |
| Custom chunking algorithm | Deterministic chunking with stable IDs required by spec | Generic chunking libraries may not provide stable IDs across runs |
